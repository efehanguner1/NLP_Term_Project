{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_term_project_final_submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version\n",
        "#!pip install q keras==2.6.0\n",
        "#!pip install q keras==2.0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asSw32XExhpn",
        "outputId": "4bedf84f-de54-4a67-ab22-05d38438844c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Currently selected TF version: 2.x\n",
            "Available versions:\n",
            "* 1.x\n",
            "* 2.x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import typing\n",
        "from typing import Any, Tuple"
      ],
      "metadata": {
        "id": "zl_v5F-_ulZL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DaKFVJa8leYT",
        "outputId": "53582b59-92f2-404c-e8ab-7cc02486b4f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 10000 ## limitation because of hardware constrainsts\n",
        "max_len = 200 ## char limitation for sentences"
      ],
      "metadata": {
        "id": "SUcPo_nQmubJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\"Turkish Noise Data/tr.train\"\n",
        "noisy_train = []\n",
        "noisy_chars = set()\n",
        "normal_train = []\n",
        "normal_chars = set()\n",
        "with open('/content/drive/My Drive/Turkish_Noise_Data/tr.train', \"r\", encoding=\"utf-8\") as f:    ##chek if you need eos and sos for both inp and tgt\n",
        "    lines = f.read().split(\"\\n\") \n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        #sentence = \"\\t\" + line + \"\\n\"  ## \\t is the start char and \\n is the end char\n",
        "        sentence = line\n",
        "        if len(sentence) <= max_len:\n",
        "          normal_train.append(sentence)\n",
        "        for char in sentence:\n",
        "          if char not in normal_chars:\n",
        "            normal_chars.add(char)\n",
        "\n",
        "with open('/content/drive/My Drive/Turkish_Noise_Data/tr_noisy.train', \"r\", encoding=\"utf-8\") as f:  \n",
        "    lines = f.read().split(\"\\n\") \n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        #sentence = \"\\t\" + line + \"\\n\"  ## \\t is the start char and \\n is the end char\n",
        "        sentence = line\n",
        "        if len(sentence) <= max_len:\n",
        "          noisy_train.append(sentence)\n",
        "        for char in sentence:\n",
        "          if char not in noisy_chars:\n",
        "            noisy_chars.add(char)\n",
        "\n",
        "\n",
        "print(noisy_chars) ###"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0pb92EbRZoo",
        "outputId": "fffb5a68-3b44-4ca0-fa2e-f0d8aaa1c6a7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'/', 'ğ', 'h', '@', '9', '[', 'ı', \"'\", '$', '`', 'ñ', 'ü', '2', 'a', ',', 's', '”', '©', 'x', 't', '!', 'm', 'w', '1', 'l', '&', 'u', 'k', ' ', 'á', 'd', 'z', 'b', 'é', '%', 'y', 'j', 'v', '(', 'c', '3', '♫', 'û', '#', 'î', 'o', '0', '7', 'f', 'n', '-', 'ç', 'q', 'í', ']', '4', '8', ';', '5', '’', ')', 'i', 'r', 'ö', 'ş', '.', '~', 'ó', '“', 'p', 'â', '?', 'g', '6', ':', 'e', '—'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(noisy_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vyJdzsuSWGoi",
        "outputId": "aa4e5cb0-d594-4d5e-8fde-3917cdddac2e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "biz diplomatlar , tejletler arasındaki natışmjlar ve sorunları çöömek için eğitildiç .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "noisy_chars = (list(noisy_chars))\n",
        "normal_chars = (list(normal_chars))\n",
        "num_encoder_tokens = len(noisy_chars)\n",
        "num_decoder_tokens = len(normal_chars)\n",
        "max_encoder_seq_length = max([len(txt) for txt in noisy_train])\n",
        "max_decoder_seq_length = max([len(txt) for txt in normal_train])\n",
        "\n",
        "input_token_index = dict([(char, i) for i, char in enumerate(noisy_chars)])\n",
        "target_token_index = dict([(char, i) for i, char in enumerate(normal_chars)])\n",
        "\n",
        "input_token_index_rev = dict([(i, char) for i, char in enumerate(noisy_chars)])\n",
        "target_token_index_rev = dict([(i, char) for i, char in enumerate(normal_chars)])"
      ],
      "metadata": {
        "id": "UGSzFjzj4inb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_token_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtDZN0BPGHZt",
        "outputId": "b03ae381-aaf9-48fb-badb-d388217a3bcc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'/': 0, 'ğ': 1, 'h': 2, '@': 3, '9': 4, '[': 5, 'ı': 6, \"'\": 7, '$': 8, '`': 9, 'ñ': 10, 'ü': 11, '2': 12, 'a': 13, ',': 14, 's': 15, '”': 16, '©': 17, 'x': 18, 't': 19, '!': 20, 'm': 21, 'w': 22, '1': 23, 'l': 24, '&': 25, 'u': 26, 'k': 27, ' ': 28, 'á': 29, 'd': 30, 'z': 31, 'b': 32, 'é': 33, '%': 34, 'y': 35, 'j': 36, 'v': 37, '(': 38, 'c': 39, '3': 40, '♫': 41, 'û': 42, '#': 43, 'î': 44, 'o': 45, '0': 46, '7': 47, 'f': 48, 'n': 49, '-': 50, 'ç': 51, 'q': 52, 'í': 53, ']': 54, '4': 55, '8': 56, ';': 57, '5': 58, '’': 59, ')': 60, 'i': 61, 'r': 62, 'ö': 63, 'ş': 64, '.': 65, '~': 66, 'ó': 67, '“': 68, 'p': 69, 'â': 70, '?': 71, 'g': 72, '6': 73, ':': 74, 'e': 75, '—': 76}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(noisy_train))\n",
        "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
        "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
        "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
        "print(\"Max sequence length for outputs:\", max_decoder_seq_length)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_Mfd4Qa34IM",
        "outputId": "0788f4b9-a5ae-4e29-b961-7274dfa2742a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9118\n",
            "Number of unique input tokens: 77\n",
            "Number of unique output tokens: 77\n",
            "Max sequence length for inputs: 200\n",
            "Max sequence length for outputs: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64  # Batch size for training. ##was 16\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "num_units = 128  # Latent dimensionality\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "ZLntnp19hZHD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sent,index_dict):\n",
        "  arr = []\n",
        "  for char in sent:\n",
        "    arr.append(index_dict[char])\n",
        "  return arr\n",
        "\n",
        "def toText(arr,index_dict):\n",
        "  sent = \"\"\n",
        "  for token in arr:\n",
        "    sent += index_dict[token]\n",
        "  return sent"
      ],
      "metadata": {
        "id": "e5MOdPXg-syE"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i,line in enumerate(noisy_train):\n",
        "#   noisy_train[i] = tokenize(line,input_token_index)\n",
        "#   while len(noisy_train[i]) < max_encoder_seq_length:\n",
        "#     noisy_train[i].append(input_token_index['\\n'])\n",
        "\n",
        "# for i,line in enumerate(normal_train):\n",
        "#   normal_train[i] = tokenize(line,target_token_index)\n",
        "#   while len(normal_train[i]) < max_decoder_seq_length:\n",
        "#     normal_train[i].append(target_token_index['\\n'])"
      ],
      "metadata": {
        "id": "O9SJ2Aemurv6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(noisy_train[0])\n",
        "# print(toText(noisy_train[0],input_token_index_rev))"
      ],
      "metadata": {
        "id": "kkzpcgwV_nEw"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def char_std(text):\n",
        "  # Split accecented characters.\n",
        "  #text = tf_text.normalize_utf8(text, 'NFKD')\n",
        "  #text = tf.strings.lower(text)\n",
        "  # Keep space, a to z, and select punctuation.\n",
        "  #text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
        "  # Add spaces around punctuation.\n",
        "  #text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
        "  # Strip whitespace.\n",
        "  text = tf.strings.strip(text)\n",
        "\n",
        "  text = tf.strings.join(['\\t', text, '\\n'], separator='')\n",
        "  return text"
      ],
      "metadata": {
        "id": "JLqvYSSbWrZU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_vocab_size = 5000\n",
        "\n",
        "input_text_processor = tf.keras.layers.TextVectorization(\n",
        "    standardize=char_std,\n",
        "    split = 'character',\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "input_text_processor.adapt(noisy_train)\n",
        "input_text_processor.get_vocabulary()  ## test vocab\n",
        "\n",
        "output_text_processor  = tf.keras.layers.TextVectorization(\n",
        "    standardize=char_std,\n",
        "    split = 'character',\n",
        "    max_tokens=max_vocab_size)\n",
        "\n",
        "output_text_processor.adapt(normal_train)"
      ],
      "metadata": {
        "id": "sXbvFlh7upSP"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((noisy_train,normal_train)).shuffle(len(noisy_train))\n",
        "dataset = dataset.batch(batch_size)\n",
        "\n",
        "#example_input_batch, example_target_batch =  dataset.take(1)\n",
        "\n",
        "for example_input_batch, example_target_batch in dataset.take(2):\n",
        "  example_input_batch = example_input_batch\n",
        "  example_target_batch = example_target_batch\n",
        "  print(example_input_batch[0])\n",
        "  example_tokens = input_text_processor(example_input_batch)\n",
        "  print(example_tokens[0])\n",
        "  input_vocab = np.array(input_text_processor.get_vocabulary())\n",
        "  tokens = input_vocab[example_tokens[0].numpy()]\n",
        "  print(''.join(tokens))\n",
        "  print(example_target_batch[0])\n",
        "  break"
      ],
      "metadata": {
        "id": "z6qXrIQK3tEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad078b33-03e5-4885-bb4a-270545365be4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'ge\\xc3\\xa7dnlerde \\xc3\\xb6\\xc4\\x9frmndi\\xc3\\xb6 ; minnettarz\\xc4\\xb1k ve olumlava negatif etkile\\xc5\\x9fimleri dengeliyormu\\xc5\\x9f .', shape=(), dtype=string)\n",
            "tf.Tensor(\n",
            "[24 25  4 26 11  7  8  4  6 11  4  2 33 30  6 12  7 11  5 33  2 51  2 12\n",
            "  5  7  7  4 16 16  3  6 19 10  9  2 27  4  2 17  8 14 12  8  3 27  3  2\n",
            "  7  4 25  3 16  5 35  2  4 16  9  5  8  4 22  5 12  8  4  6  5  2 11  4\n",
            "  7 25  4  8  5 13 17  6 12 14 22  2 21 23  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0], shape=(190,), dtype=int64)\n",
            "\tgeçdnlerde öğrmndiö ; minnettarzık ve olumlava negatif etkileşimleri dengeliyormuş .\n",
            "\n",
            "tf.Tensor(b'ge\\xc3\\xa7enlerde \\xc3\\xb6\\xc4\\x9frendim ; minnettarl\\xc4\\xb1k ve olumlama negatif etkile\\xc5\\x9fimleri dengeliyormu\\xc5\\x9f .', shape=(), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 1\n",
        "units = 128 ##latent dim was 1024"
      ],
      "metadata": {
        "id": "qHNZ65dBNhjL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.enc_units = enc_units\n",
        "    self.input_vocab_size = input_vocab_size\n",
        "\n",
        "    # The embedding layer converts tokens to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # The GRU RNN layer processes those vectors sequentially.\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',dropout=0.2)\n",
        "\n",
        "  def call(self, tokens, state=None):\n",
        "\n",
        "    # 2. The embedding layer looks up the embedding for each token.\n",
        "    vectors = self.embedding(tokens)\n",
        "\n",
        "    # 3. The GRU processes the embedding sequence.\n",
        "    #    output shape: (batch, s, enc_units)\n",
        "    #    state shape: (batch, enc_units)\n",
        "    output, state = self.gru(vectors, initial_state=state)\n",
        "    # 4. Returns the new sequence and its state.\n",
        "    return output, state"
      ],
      "metadata": {
        "id": "ARSoacDAm1Ex"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ],
      "metadata": {
        "id": "Zt9z2aYVWknJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super().__init__()\n",
        "    # For Eqn. (4), the  Bahdanau attention\n",
        "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
        "\n",
        "    self.attention = tf.keras.layers.AdditiveAttention()\n",
        "\n",
        "  def call(self, query, value, mask):\n",
        "\n",
        "    # From Eqn. (4), `W1@ht`.\n",
        "    w1_query = self.W1(query)\n",
        "\n",
        "    # From Eqn. (4), `W2@hs`.\n",
        "    w2_key = self.W2(value)\n",
        "\n",
        "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
        "    value_mask = mask\n",
        "\n",
        "    context_vector, attention_weights = self.attention(\n",
        "        inputs = [w1_query, value, w2_key],\n",
        "        mask=[query_mask, value_mask],\n",
        "        return_attention_scores = True,\n",
        "    )\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "metadata": {
        "id": "drF-MYf9m4Gn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = BahdanauAttention(units)"
      ],
      "metadata": {
        "id": "b4lhxhYZWaO5"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.dec_units = dec_units\n",
        "    self.output_vocab_size = output_vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "\n",
        "    # For Step 1. The embedding layer convets token IDs to vectors\n",
        "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
        "                                               embedding_dim)\n",
        "\n",
        "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   # Return the sequence and state\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform',dropout=0.2)\n",
        "\n",
        "    # For step 3. The RNN output will be the query for the attention layer.\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
        "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
        "                                    use_bias=False)\n",
        "\n",
        "    # For step 5. This fully connected layer produces the logits for each\n",
        "    # output token.\n",
        "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)"
      ],
      "metadata": {
        "id": "xGVM8MNh6xh8"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderInput(typing.NamedTuple):\n",
        "  new_tokens: Any\n",
        "  enc_output: Any\n",
        "  mask: Any\n",
        "\n",
        "class DecoderOutput(typing.NamedTuple):\n",
        "  logits: Any\n",
        "  attention_weights: Any"
      ],
      "metadata": {
        "id": "Ph5iVOJCm6AL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def call(self,\n",
        "         inputs: DecoderInput,\n",
        "         state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
        "\n",
        "  # if state is not None:\n",
        "  #   shape_checker(state, ('batch', 'dec_units'))\n",
        "\n",
        "  # Step 1. Lookup the embeddings\n",
        "  vectors = self.embedding(inputs.new_tokens)\n",
        "\n",
        "  # Step 2. Process one step with the RNN\n",
        "  rnn_output, state = self.gru(vectors, initial_state=state)\n",
        "\n",
        "  # Step 3. Use the RNN output as the query for the attention over the\n",
        "  # encoder output.\n",
        "  context_vector, attention_weights = self.attention(\n",
        "      query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
        "\n",
        "  # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
        "  #     [ct; ht] shape: (batch t, value_units + query_units)\n",
        "  context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
        "\n",
        "  # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
        "  attention_vector = self.Wc(context_and_rnn_output)\n",
        "\n",
        "  # Step 5. Generate logit predictions:\n",
        "  logits = self.fc(attention_vector)\n",
        "\n",
        "  return DecoderOutput(logits, attention_weights), state"
      ],
      "metadata": {
        "id": "JNr-sRVQm71j"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Decoder.call = call"
      ],
      "metadata": {
        "id": "rsu5_KkIm9t5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                  embedding_dim, units)"
      ],
      "metadata": {
        "id": "CcU-PpDFm_Wf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaskedLoss(tf.keras.losses.Loss):\n",
        "  def __init__(self):\n",
        "    self.name = 'masked_loss'\n",
        "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "        from_logits=True, reduction='none')\n",
        "\n",
        "  def __call__(self, y_true, y_pred):\n",
        "\n",
        "    # Calculate the loss for each item in the batch.\n",
        "    loss = self.loss(y_true, y_pred)\n",
        "\n",
        "    # Mask off the losses on padding.\n",
        "    mask = tf.cast(y_true != 0, tf.float32)\n",
        "    loss *= mask\n",
        "\n",
        "    # Return the total.\n",
        "    return tf.reduce_sum(loss)"
      ],
      "metadata": {
        "id": "lqTGojUWnA0i"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainModel(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units,\n",
        "               input_text_processor,\n",
        "               output_text_processor, \n",
        "               use_tf_function=True):\n",
        "    super().__init__()\n",
        "    # Build the encoder and decoder\n",
        "    encoder = Encoder(input_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "    decoder = Decoder(output_text_processor.vocabulary_size(),\n",
        "                      embedding_dim, units)\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "    self.use_tf_function = use_tf_function\n",
        "\n",
        "  def train_step(self, inputs):\n",
        "    if self.use_tf_function:\n",
        "      return self._tf_train_step(inputs)\n",
        "    else:\n",
        "      return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "l1Xhd7CYnDEZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _preprocess(self, input_text, target_text):\n",
        "\n",
        "  # Convert the text to token IDs\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  target_tokens = self.output_text_processor(target_text)\n",
        "\n",
        "  # Convert IDs to masks.\n",
        "  input_mask = input_tokens != 0\n",
        "\n",
        "  target_mask = target_tokens != 0\n",
        "\n",
        "  return input_tokens, input_mask, target_tokens, target_mask"
      ],
      "metadata": {
        "id": "9-UCaF3kwf0w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainModel._preprocess = _preprocess"
      ],
      "metadata": {
        "id": "e5SJQ5cDwr3S"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _train_step(self, inputs):\n",
        "  input_text, target_text = inputs   ## change here\n",
        "\n",
        "  (input_tokens, input_mask,\n",
        "   target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
        "\n",
        "  max_target_length = tf.shape(target_tokens)[1]\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    # Encode the input\n",
        "    enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "    # Initialize the decoder's state to the encoder's final state.\n",
        "    # This only works if the encoder and decoder have the same number of\n",
        "    # units.\n",
        "    dec_state = enc_state\n",
        "    loss = tf.constant(0.0)\n",
        "\n",
        "    for t in tf.range(max_target_length-1):\n",
        "      # Pass in two tokens from the target sequence:\n",
        "      # 1. The current input to the decoder.\n",
        "      # 2. The target for the decoder's next prediction.\n",
        "      new_tokens = target_tokens[:, t:t+2]\n",
        "      step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
        "                                             enc_output, dec_state)\n",
        "      loss = loss + step_loss\n",
        "\n",
        "    # Average the loss over all non padding tokens.\n",
        "    average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
        "\n",
        "  # Apply an optimization step\n",
        "  variables = self.trainable_variables \n",
        "  gradients = tape.gradient(average_loss, variables)\n",
        "  self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  # Return a dict mapping metric names to current value\n",
        "  return {'batch_loss': average_loss}"
      ],
      "metadata": {
        "id": "7kE3gNn0w6Vq"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainModel._train_step = _train_step"
      ],
      "metadata": {
        "id": "jfbTUjyAyo0V"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
        "  input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
        "\n",
        "  # Run the decoder one step.\n",
        "  decoder_input = DecoderInput(new_tokens=input_token,\n",
        "                               enc_output=enc_output,\n",
        "                               mask=input_mask)\n",
        "\n",
        "  dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
        "\n",
        "  # `self.loss` returns the total for non-padded tokens\n",
        "  y = target_token\n",
        "  y_pred = dec_result.logits\n",
        "  step_loss = self.loss(y, y_pred)\n",
        "\n",
        "  return step_loss, dec_state"
      ],
      "metadata": {
        "id": "e7mJtvhSytTE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainModel._loop_step = _loop_step"
      ],
      "metadata": {
        "id": "ZWMybNZXy3Ru"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "charModel = TrainModel(\n",
        "    embedding_dim, units,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        "    use_tf_function=False)\n",
        "\n",
        "# Configure the loss and optimizer\n",
        "charModel.compile(\n",
        "    optimizer=tf.optimizers.Adam(),\n",
        "    loss=MaskedLoss(),\n",
        ")"
      ],
      "metadata": {
        "id": "MnCMY22R2-5I"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## loss should start near this\n",
        "np.log(output_text_processor.vocabulary_size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5R6i_3wy-AY",
        "outputId": "25ad231b-d012-4eae-e4d4-d7f9b97e792b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.3694478524670215"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(charModel.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPh4SaXmzJqp",
        "outputId": "4c376c54-3525-4d5e-abd4-58b4df30d6fa"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.31936>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.3121853>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.302684>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.2896023>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.271751>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.2476535>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.2153234>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.172043>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.11411>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=4.036419>}\n",
            "\n",
            "CPU times: user 37.7 s, sys: 1.01 s, total: 38.7 s\n",
            "Wall time: 42.2 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
        "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
        "def _tf_train_step(self, inputs):\n",
        "  return self._train_step(inputs)"
      ],
      "metadata": {
        "id": "NsHxXUxs5S8R"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TrainModel._tf_train_step = _tf_train_step"
      ],
      "metadata": {
        "id": "NOYAyfKs5axg"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "charModel.use_tf_function = True"
      ],
      "metadata": {
        "id": "N3z8sszG5cxz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "charModel.train_step([example_input_batch, example_target_batch])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbq9qHNH5h_o",
        "outputId": "26bf6a6a-4c0a-4e17-c46d-1b30ef0a305a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.932152>}"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "for n in range(10):\n",
        "  print(charModel.train_step([example_input_batch, example_target_batch]))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7ztn72W5jvW",
        "outputId": "fe81dcab-dabe-46fa-a5fb-14e4008bffb4"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.7933333>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.617003>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.4459004>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.5871983>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.470313>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.343694>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.3025699>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.2973015>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.296774>}\n",
            "{'batch_loss': <tf.Tensor: shape=(), dtype=float32, numpy=3.2914217>}\n",
            "\n",
            "CPU times: user 39.5 s, sys: 4.85 s, total: 44.4 s\n",
            "Wall time: 24.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BatchLogs(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, key):\n",
        "    self.key = key\n",
        "    self.logs = []\n",
        "\n",
        "  def on_train_batch_end(self, n, logs):\n",
        "    self.logs.append(logs[self.key])\n",
        "\n",
        "batch_loss = BatchLogs('batch_loss')"
      ],
      "metadata": {
        "id": "d0QTZEkT78W7"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "charModel.fit(dataset, epochs=20,\n",
        "                     callbacks=[batch_loss])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JutKVEKG7_hE",
        "outputId": "28094917-ba03-4e84-f0d7-25ac4941e4d3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "143/143 [==============================] - 371s 3s/step - batch_loss: 3.1252\n",
            "Epoch 2/20\n",
            "143/143 [==============================] - 368s 3s/step - batch_loss: 2.8651\n",
            "Epoch 3/20\n",
            "143/143 [==============================] - 359s 3s/step - batch_loss: 2.4808\n",
            "Epoch 4/20\n",
            "143/143 [==============================] - 360s 3s/step - batch_loss: 2.3918\n",
            "Epoch 5/20\n",
            "143/143 [==============================] - 361s 3s/step - batch_loss: 2.3353\n",
            "Epoch 6/20\n",
            "143/143 [==============================] - 359s 3s/step - batch_loss: 2.2868\n",
            "Epoch 7/20\n",
            "143/143 [==============================] - 360s 3s/step - batch_loss: 2.2362\n",
            "Epoch 8/20\n",
            "143/143 [==============================] - 363s 3s/step - batch_loss: 2.1919\n",
            "Epoch 9/20\n",
            "143/143 [==============================] - 364s 3s/step - batch_loss: 2.1516\n",
            "Epoch 10/20\n",
            "143/143 [==============================] - 367s 3s/step - batch_loss: 2.1073\n",
            "Epoch 11/20\n",
            "143/143 [==============================] - 372s 3s/step - batch_loss: 2.0763\n",
            "Epoch 12/20\n",
            "143/143 [==============================] - 363s 3s/step - batch_loss: 2.0533\n",
            "Epoch 13/20\n",
            "143/143 [==============================] - 364s 3s/step - batch_loss: 2.0319\n",
            "Epoch 14/20\n",
            "143/143 [==============================] - 364s 3s/step - batch_loss: 2.0128\n",
            "Epoch 15/20\n",
            "143/143 [==============================] - 363s 3s/step - batch_loss: 1.9984\n",
            "Epoch 16/20\n",
            "143/143 [==============================] - 357s 2s/step - batch_loss: 1.9832\n",
            "Epoch 17/20\n",
            "143/143 [==============================] - 361s 3s/step - batch_loss: 1.9696\n",
            "Epoch 18/20\n",
            "143/143 [==============================] - 360s 3s/step - batch_loss: 1.9558\n",
            "Epoch 19/20\n",
            "143/143 [==============================] - 365s 3s/step - batch_loss: 1.9427\n",
            "Epoch 20/20\n",
            "143/143 [==============================] - 365s 3s/step - batch_loss: 1.9295\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ae00af310>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class charModel_Main(tf.Module):\n",
        "\n",
        "  def __init__(self, encoder, decoder, input_text_processor,\n",
        "               output_text_processor):\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.input_text_processor = input_text_processor\n",
        "    self.output_text_processor = output_text_processor\n",
        "\n",
        "    self.output_token_string_from_index = (\n",
        "        tf.keras.layers.StringLookup(\n",
        "            vocabulary=output_text_processor.get_vocabulary(),\n",
        "            mask_token='',\n",
        "            invert=True))\n",
        "\n",
        "    # The output should never generate padding, unknown, or start.\n",
        "    index_from_string = tf.keras.layers.StringLookup(\n",
        "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
        "    token_mask_ids = index_from_string(['', '[UNK]', '\\t']).numpy()\n",
        "\n",
        "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
        "    token_mask[np.array(token_mask_ids)] = True\n",
        "    self.token_mask = token_mask\n",
        "\n",
        "    self.start_token = index_from_string(tf.constant('\\t'))\n",
        "    self.end_token = index_from_string(tf.constant('\\n'))"
      ],
      "metadata": {
        "id": "wBNVH3APgLnp"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "charModel_trained = charModel_Main(\n",
        "    encoder=charModel.encoder,\n",
        "    decoder=charModel.decoder,\n",
        "    input_text_processor=input_text_processor,\n",
        "    output_text_processor=output_text_processor,\n",
        ")"
      ],
      "metadata": {
        "id": "7Us1n2JdgM7r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc9cbf40-c4b6-4251-95c5-85ef400f5a75"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_to_text(self, result_tokens):\n",
        "  result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
        "\n",
        "  result_text = tf.strings.reduce_join(result_text_tokens,\n",
        "                                       axis=1, separator='')\n",
        "\n",
        "  result_text = tf.strings.strip(result_text)\n",
        "  return result_text"
      ],
      "metadata": {
        "id": "_9QD4zGQkQfK"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "charModel_Main.tokens_to_text = tokens_to_text"
      ],
      "metadata": {
        "id": "vW6kxc-kkQ-d"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sample(self, logits, temperature):\n",
        "  # 't' is usually 1 here.\n",
        "\n",
        "  token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
        "\n",
        "  # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
        "  logits = tf.where(self.token_mask, -np.inf, logits)\n",
        "\n",
        "  if temperature == 0.0:\n",
        "    new_tokens = tf.argmax(logits, axis=-1)\n",
        "  else: \n",
        "    logits = tf.squeeze(logits, axis=1)\n",
        "    new_tokens = tf.random.categorical(logits/temperature,\n",
        "                                        num_samples=1)\n",
        "\n",
        "  return new_tokens"
      ],
      "metadata": {
        "id": "j0AikyeMke9Z"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "charModel_Main.sample = sample"
      ],
      "metadata": {
        "id": "X1-g5uhlkfdk"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_unrolled(self,\n",
        "                       input_text, *,\n",
        "                       max_length=50,\n",
        "                       return_attention=True,\n",
        "                       temperature=1.0):\n",
        "  batch_size = tf.shape(input_text)[0]\n",
        "  input_tokens = self.input_text_processor(input_text)\n",
        "  enc_output, enc_state = self.encoder(input_tokens)\n",
        "\n",
        "  dec_state = enc_state\n",
        "  new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
        "\n",
        "  result_tokens = []\n",
        "  attention = []\n",
        "  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
        "\n",
        "  for _ in range(max_length):\n",
        "    dec_input = DecoderInput(new_tokens=new_tokens,\n",
        "                             enc_output=enc_output,\n",
        "                             mask=(input_tokens!=0))\n",
        "    \n",
        "    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
        "\n",
        "    attention.append(dec_result.attention_weights)\n",
        "\n",
        "    new_tokens = self.sample(dec_result.logits, temperature)\n",
        "\n",
        "    # If a sequence produces an `end_token`, set it `done`\n",
        "    done = done | (new_tokens == self.end_token)\n",
        "    # Once a sequence is done it only produces 0-padding.\n",
        "    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
        "\n",
        "    # Collect the generated tokens\n",
        "    result_tokens.append(new_tokens)\n",
        "\n",
        "    if tf.executing_eagerly() and tf.reduce_all(done):\n",
        "      break\n",
        "\n",
        "  # Convert the list of generates token ids to a list of strings.\n",
        "  result_tokens = tf.concat(result_tokens, axis=-1)\n",
        "  result_text = self.tokens_to_text(result_tokens)\n",
        "\n",
        "  if return_attention:\n",
        "    attention_stack = tf.concat(attention, axis=1)\n",
        "    return {'text': result_text, 'attention': attention_stack}\n",
        "  else:\n",
        "    return {'text': result_text}"
      ],
      "metadata": {
        "id": "fDFetKYIkgtV"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "charModel_Main.translate = translate_unrolled"
      ],
      "metadata": {
        "id": "Q4ohnE-lkwEm"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
        "def tf_translate(self, input_text):\n",
        "  return self.translate(input_text)\n",
        "\n",
        "charModel_Main.tf_translate = tf_translate"
      ],
      "metadata": {
        "id": "0rP7JhWJkzK0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "input_text = tf.constant([\n",
        "    've lize şunu söyleyebilirim , ajandamız dolu .', \n",
        "    've kim şeröristlerll konuşur ki ?', \n",
        "])\n",
        "\n",
        "result = charModel_trained.tf_translate(\n",
        "    input_text = input_text)\n",
        "\n",
        "print(result['text'][0].numpy().decode())\n",
        "print(result['text'][1].numpy().decode())\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nm01mAZL0mwV",
        "outputId": "53084304-dff0-4867-99c7-599e2dd4377f"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ve çabalir heyilke istalak de bizi urlarısı olaşıy\n",
            "ve bünda gerüyetin 3 '9kre kok geretteye k.ssiniye\n",
            "\n",
            "CPU times: user 108 ms, sys: 8.78 ms, total: 117 ms\n",
            "Wall time: 80.1 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_output_tokens = tf.random.uniform(\n",
        "    shape=[5, 2], minval=0, dtype=tf.int64,\n",
        "    maxval=output_text_processor.vocabulary_size())\n",
        "charModel_trained.tokens_to_text(example_output_tokens).numpy()"
      ],
      "metadata": {
        "id": "PaXfe9vZ7i6T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eea07fd1-5f14-4a32-8188-17d6919cdd0e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'zl', b'7:', b'jf', b'\\xe2\\x80\\x9ca', b',w'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(charModel_trained, 'drive/MyDrive/charBased',\n",
        "                    signatures={'serving_default': charModel_trained.tf_translate})"
      ],
      "metadata": {
        "id": "Q8CWWDgJlCT5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bba66d7-3628-4d49-cf17-1fd5a903af7d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as encoder_1_layer_call_fn, encoder_1_layer_call_and_return_conditional_losses, decoder_1_layer_call_fn, decoder_1_layer_call_and_return_conditional_losses, embedding_2_layer_call_fn while saving (showing 5 of 24). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: drive/MyDrive/charBased/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: drive/MyDrive/charBased/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "charModel_trained.save('/drive/myDrive')"
      ],
      "metadata": {
        "id": "9PIvY--vWIEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## load test data\n",
        "#\"Turkish Noise Data/tr.test\"\n",
        "noisy_test = []\n",
        "#noisy_chars = set()\n",
        "normal_test = []\n",
        "#normal_chars = set()\n",
        "with open('/content/drive/My Drive/Turkish_Noise_Data/tr.test', \"r\", encoding=\"utf-8\") as f:  \n",
        "    lines = f.read().split(\"\\n\") \n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        #sentence = \"\\t\" + line + \"\\n\"  ## \\t is the start char and \\n is the end char\n",
        "        sentence = line\n",
        "        normal_test.append(sentence)\n",
        "        # for char in sentence:\n",
        "        #   if char not in normal_chars:\n",
        "        #     normal_chars.add(char)\n",
        "\n",
        "with open('/content/drive/My Drive/Turkish_Noise_Data/tr_noisy.test', \"r\", encoding=\"utf-8\") as f:  \n",
        "    lines = f.read().split(\"\\n\") \n",
        "    for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "        noisy_test.append(line)\n",
        "        # for char in line:\n",
        "        #   if char not in noisy_chars:\n",
        "        #     noisy_chars.add(char)"
      ],
      "metadata": {
        "id": "qByRP9gZkQq3"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,sentence in enumerate(noisy_test):\n",
        "  tp = 0\n",
        "  fp = 0\n",
        "  tn = 0\n",
        "  fn = 0\n",
        "\n",
        "  decoded_sentence = charModel_trained.tf_translate(tf.constant([sentence]))['text'][0].numpy().decode()\n",
        "  print(\"-\")\n",
        "  print(\"Input sentence:\", sentence)\n",
        "  print(\"Decoded sentence:\", decoded_sentence)\n",
        "  print(\"Original sentence:\", normal_test[i])\n",
        "  \n",
        "  for t,char in enumerate(normal_test[i]):\n",
        "    if char == decoded_sentence[t] and sentence[t] != char:\n",
        "      tp+=1\n",
        "    elif char != decoded_sentence[t] and sentence[t] == char:\n",
        "      fp+=1\n",
        "    elif char != decoded_sentence[t] and sentence[t] != char:\n",
        "      fn+=1\n",
        "\n",
        "    if(t+1 == len(decoded_sentence)) :  ## if len is exceeded\n",
        "      break\n",
        "\n",
        "  Recall = tp/(tp+fn)\n",
        "  Precision = tp/(tp+fp)\n",
        "  F1 = 2*((Precision*Recall)/(Precision+Recall))\n",
        "  print(\"Recall is: \"+ str(Recall))\n",
        "  print(\"Precision is: \"+ str(Precision))\n",
        "  print(\"F1 Score is: \"+ str(F1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "STwDQ24tkUal",
        "outputId": "3a9cb357-ccb4-4322-9e94-9b16969c90bf"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-\n",
            "Input sentence: bu senenin sonunda , yaklaşık bir milyar insan sosynl ağ siielerini aktif olarjk kullanıyor olacak .\n",
            "Decoded sentence: bacliri atıyan abdi başım barik girzeyi nevrim kıl\n",
            "Original sentence: bu senenin sonunda , yaklaşık bir milyar insan sosyal ağ sitelerini aktif olarak kullanıyor olacak .\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-ed5012b0dcc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mRecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mPrecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtp\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mF1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mRecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mRecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ]
    }
  ]
}